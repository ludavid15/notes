I"≠<p>The thrilling conclusion to a three part series on music and artifical intelligence!</p>

<!--more-->

<p>In the second post on AI and Music, we discussed why it‚Äôs difficult to generate music with AI. Along the way, we presented different learning <em>archictures</em>, and how they were applied to the problem through different <em>strategies</em>. Here is a quick summary of everything we went through:</p>

<h3 id="list-of-architectures">List of Architectures</h3>

<ol>
  <li>Feedforward</li>
  <li>Autoencoder</li>
  <li>Variational Autoencoder</li>
  <li>Restricted Boltzman Machine</li>
  <li>Recurrent Networks</li>
  <li>Convolutional Networks</li>
  <li>Conditioning Convolutional Networks</li>
  <li>Generative Adversarial Networks</li>
  <li>Reinforcement Learning</li>
</ol>

<h3 id="list-of-strategies">List of Strategies</h3>

<ol>
  <li>Single step feedforward</li>
  <li>Decoder feedforward</li>
  <li>Sampling based methods</li>
  <li>Iterative feedforward</li>
  <li>Input manipulation</li>
  <li>Reinforcement</li>
  <li>Unit Selection</li>
</ol>

<h3 id="recurrent-vs-convolutional-networks">Recurrent vs Convolutional Networks</h3>

<p>Convolutional networks have not been explored thoroughly as a way to generate or recognize music. Maybe this is because music representation is so complex that it‚Äôs difficult to visualize how a CNN would apply.</p>

<p>That being said, convolutional networks do have two advantages:</p>
<ol>
  <li>Faster to train and easier to parallelize</li>
  <li>By nature of their implementation, convolutional networks increase the volume of data.</li>
</ol>

<h3 id="transfer-learning">Transfer Learning</h3>

<p>Transfer learning for music is one area that really hasn‚Äôt been explored in depth, but which could be enormously useful, if we use the (more developed) subject of image generation as a sign of things to come.</p>

<h3 id="open-questions">Open Questions</h3>

<ol>
  <li>
    <p>Why do some training sets transpose all songs into a single key, while other training sets transpose all songs into every key? Is there a difference?</p>
  </li>
  <li>
    <p>Can we use a lead sheet as the input, with a polyphonic melody/solo as the output? Maybe the pianist can be the conditional input (i.e. toggle style for different players)</p>
  </li>
  <li>
    <p>Can we apply Elgammal‚Äôs CAN, to create jazz solos that seem real, but which are not easily classified into an existing composer?</p>
  </li>
</ol>

<h3 id="project-ideas">Project Ideas</h3>

<h4 id="objective">Objective</h4>

<p>Train a network to produce jazz solos, based on a lead sheet as input.</p>

<h4 id="input">Input</h4>

<p>The input should be a lead sheet, featuring melody, and chords. Since lead sheets are of varying length, we will need an RNN, possibly an LSTM for the architecture. There shall also be a label for the performer.</p>

<h4 id="output">Output</h4>

<p>The output should be a polyphonic piano part (i.e. one instrument, with multiple notes/voices).</p>

<h4 id="limitations">Limitations</h4>

<p>It will be difficult to find one standard that has been performed by many different jazz musicians. If we‚Äôd like to apply different styles, it probably makes more sense to train a network that identifies styles independent of the lead sheet. However, it‚Äôs possible that this will result in the classifier learning to associate melodies to musicians, and not styles.</p>

<h4 id="architecture">Architecture</h4>

<p>The architecture shall be an LSTM, which takes as input a lead sheet and composer and outputs a jazz solo for that lead sheet. This output is then trained against a classification network, which attempts to distinguish between real and generated content. The classification network also needs to be an RNN/LSTM to handle the variable input length.</p>

<p>The optimizer the will push the generative network to produce solos that are believed to be real, but difficult to classify.</p>

<p>The output of the generative network shall be the same as the input to the classifier.</p>

<h4 id="outline">Outline</h4>

<p>First, we need to identify a source of training data, and convert these into our input representation. This should be a challenging first step, but once completed, we will be able to tweak our architecture more easily.</p>

<p>To start, we can train the classification network independently to classify performers. The ouput of this will be a softmax function, with a threshold.</p>

<h4 id="training-data">Training Data</h4>

<p>One place to start with our training data is to simply take what‚Äôs most easily accessible. In this case, maybe the musescore database of jazz standards.</p>

<h4 id="lessons-learned">Lessons Learned</h4>

<ol>
  <li>
    <p>MIDI is a readily accessible format for storing musical information, but lacks support for extrinsic chord representation (e.g. C major 7 #9).</p>
  </li>
  <li>
    <p>This problem is potentially resolved by musicXML, which is a more explicit format and CAN record text information/notation.</p>
  </li>
  <li>
    <p><a href="https://juliamusic.github.io/JuliaMusic_documentation.jl/latest/">JuliaMusic</a> provides an easy way to create midi tracks, and the documentation goes into detail for ways to export to MuseScore for sheet music and playback.</p>
  </li>
  <li>
    <p>Flux.jl is the most popular machine learning library for Julia.</p>
  </li>
  <li>
    <p>Next step - build a Julia function for reading and exporting XML files. This input to this file should be a an XML file, and the ouput should be an array, which can serve as the input to our deep learning network.</p>
  </li>
  <li>
    <p>MusicXMl 2.0 now supports a compressed version called ‚Äú.mxl‚Äù - this brings the file size to be comparable to MIDI</p>
  </li>
  <li>
    <p>Music encoding is NOT straightforward, or maybe it is, but just not apparent to me.</p>
  </li>
  <li>
    <p>I need to remember that the goal is not to design a general AI capable of doing everything, I just need it to produce a jazz piano solo.</p>
  </li>
</ol>
:ET