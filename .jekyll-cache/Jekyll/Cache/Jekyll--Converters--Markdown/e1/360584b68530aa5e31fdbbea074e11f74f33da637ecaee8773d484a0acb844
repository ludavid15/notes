I"éS<p>This post explores a variety of tried and tested approaches to music generation with deep learning. Although each example can stand alone in theory, they are presented in an order that gradually introduces new concepts and challenges.</p>

<!--more-->

<p>The complete list of challenges is here:</p>

<ol>
  <li><em>Creatio Ex nihilo</em>  (creation from nothing)</li>
  <li>Length variability (music of different lengths)</li>
  <li>Content variability (is it deterministic?)</li>
  <li>Expressiveness (dynamics and feeling)</li>
  <li>Melody-harmony consistency</li>
  <li>Control (user ability to affect outcome)</li>
  <li>Style transfer (applicable to images, but can it apply to music?)</li>
  <li>Structure (songs formats, AABA, etc.)</li>
  <li>Originality (not just repeating information from training data)</li>
  <li>Incrementality</li>
  <li>Adaptability</li>
  <li>Explainability</li>
</ol>

<p>As we will see, some of these are at odds with one another. For example, the solution to overcoming length variability is worse at creating structure.</p>

<h3 id="example-1---introduction-with-bach">Example 1 - Introduction with Bach</h3>

<p>For the first example, we‚Äôll walk through a very simple and intuitive deep learning setup. Here are the parameters:</p>

<ol>
  <li><strong>Music Structure</strong> - 4 measures of SATB in 4/4 time</li>
  <li><strong>ML Architecture</strong> - Feedforward neural network with 1 hidden layer</li>
  <li><strong>Input</strong> - (One hot encoding for pitch at each time step) x number of input voices</li>
  <li><strong>Output</strong> - (One hot encoding for pitch at each time step) x number of output voices</li>
  <li><strong>Training Data</strong> - Bach chorales, with each voice extracted and transposed into all keys.</li>
</ol>

<p>Even before we consider the musical merit of any generated output, we can notice that there will be intrinsic limitations to this method. First, it is deterministic. For each unique input there will only ever be 1 output. Maybe this ok for something like this, but what if our input was a chord progression and our desired ouput was a melody? Many different melodies are possible for the same progression. Here we run into our first challenge - how can we generate a wide variety of content from limited inputs? (<em>creatio ex-nihilo</em> and content variability)</p>

<p>The next two examples are ways around this issue.</p>

<h3 id="example-2---decoder-feedfoward">Example 2 - Decoder Feedfoward</h3>

<p>The basic idea here is to first train an autoencoder, which if you remember, learns the important ‚Äúfeatures‚Äù in its hidden layer. To generate unique outputs, we simply turn this around - the hidden layer becomes the input (i.e. a seed). The seed is fed-forward through the decoder, and we get a unique output. Of course this is still deterministic, but it overcomes the <em>creatio ex-nihilo</em> issue. (Technically we‚Äôre not starting from <em>true nothing</em>, but then what does? Most randomly generated content, like a minecraft world, starts from a seed anyway).</p>

<blockquote>
  <p>The DeepHear system by Sun uses this approach to generate ragtime music. They use Scott Joplin songs as the training data, and a stacked autoencoder with a hidden layer size of 16. Due to the small size of this hidden layer, there is a high amount of plagiarism (around 60%).</p>
</blockquote>

<h3 id="example-3---sampling">Example 3 - Sampling</h3>

<p>Sampling methods work a little like the feedforward decoder in example 2, but instead of generating examples which are <em>deterministic</em> based on a seed, examples are randomly generated to match a probability distribution. Some famous sampling strategies include: Metropolis-Hastings algorithm, Gibbs Sampling, and block Gibbs Sampling. These methods overcome both the problem of determinism, <em>and</em> the problem of something from nothing.</p>

<blockquote>
  <p>Another way to think about sampling methods is that they are a form of generative AI that is based on stochastic models which learn probability distributions (variational autoencoders, RBM‚Äôs, etc.). For this reason, we talk about them as a different type of solution, but conceptually, they can manifest just like the decoder in example 2 (i.e. a seed is used to randomly generate examples).</p>
</blockquote>

<p>With regard to music, there are two ways to apply this type of sampling approach. The first, is in the vertical dimension, (i.e. chord - does the voicing of this chord make sense?). Recurrent Boltzman Machines are effective for this kind of work. The second is in the horizontal dimension (i.e. a melody or sequence of notes - does this sequence of notes make a coherent melody?). This kind of sampling is better achieved by an RNN.</p>

<h3 id="example-4---iterative-feedforward">Example 4 - Iterative Feedforward</h3>

<p>So far all the examples have shared a common limitation - their output is fixed in length. One way to overcome this problem is to fundamentally change the type of input and ouput. An iterative feedforward system takes some initial starting condition, and ouputs something for the next timestep. This can then be repeated indefinitely to produce music of any length.</p>

<p>An example of this approach was implemented by Eck and Schmidhuber in their 12 bar blues program. The input and output layer both have 25 nodes (13 for notes and 12 for chords) with a hidden layer of eight LSTM blocks. The hidden layer connections are as follows:</p>

<ol>
  <li>Chord block is fully connected to everything in the input and the chord output.</li>
  <li>Melody block is fully connected to everything in the input, and melody output.</li>
  <li>Chord block is recursive with itself and melody.</li>
  <li>Melody block is recursive only with itself.</li>
</ol>

<p>The asymmetric architecture is used to control the dependency of chords and melody. In this case, the chords are dependent on the melody, but not vice-versa. Also worth noting, the use of an RNN (LTSM) architecture for an interative program is not by coincidence. The nature of RNN‚Äôs lend themselves very well to this kind of setup.</p>

<h3 id="example-5---bi-directional-ltsm-for-accompaniment">Example 5 - Bi-Directional LTSM for Accompaniment</h3>

<p>RNN‚Äôs by their nature lend themselves to a recursive strategy (the ouput becomes the input); however this is not the only way to apply an RNN architecture.</p>

<p>The objective of the BLTSM architecture is to generate a progression of chords as an accompaniment to a melody.</p>

<ul>
  <li>Input encoding: one-hot for each of 12 pitches</li>
  <li>Output encoding: one-hot for each minor and major triad of 12 pitches (24 total)</li>
</ul>

<p>This particular structure is unique from many other LTSM or recurrent networks because although it is recurrent, it is not recursive (i.e. the chord output is not fed back into input). This example highlights that there is more than one way to use different architectures.</p>

<h3 id="example-6---sampling-again">Example 6 - Sampling Again</h3>

<p>In example 3, we introduced sampling as a means to create something from nothing, but sampling is also useful because it is non-deterministic. (i.e. instead of choosing the most probable solution, we can randomly choose a solution based on the probability distribution).</p>

<p>An early application of sampling is the CONCERT Bach Melody generation system by Mozer (1994). The program generates melodies with a chord accompaniment, where the chords are just another aspect of each note, alongside pitch and duration. We‚Äôll discuss each of these in order, starting first with pitch.</p>

<ul>
  <li>
    <p>The pitch of each note has three components, which are the pitch height, the (modulo) chroma circle cartesion coordinates, and the (harmonic) circle of fifths cartesian coordinates. The second two coordinates help capture the similarity of the octave, and the harmonic importance of the fifth respectively.</p>
  </li>
  <li>
    <p>The duration is subdivided into 12, to accomodate both regular divisions as well as triplets. As with the pitch, there are 3 coordinates to the duration, the absolute duration, the 1/3 beat circle, and the 1/4 beat circle.</p>
  </li>
  <li>
    <p>The chords are always structured the same, with a root, a third that can be major or minor, a fifth that can be perfect, augmented, or diminished, and an optional 7th that can be major or minor.</p>
  </li>
</ul>

<p>One of the reasons why the CONCERT program uses such a complex representation is because it pre-dates modern deep learning algorithms. Instead of allowing an archicture to learn these relationships, the authors manually design them into the representation.</p>

<h3 id="example-7---rnn-rbm-polyphany-generation">Example 7 - RNN-RBM Polyphany Generation</h3>

<p>Ok, so we‚Äôve been introducing architectures to tackle different technical problems, but we haven‚Äôt factored in anything about music yet. Music theory offers many ideas through which we can understand the structure and shape of songs, so why not incoporate them into our architecture?</p>

<p>To begin, let‚Äôs start with the basic differentiation of melody and harmony. From a music theory perspective, there are two dimensions to a song: the vertical and the horizontal. Vertical is the pairing of melody with harmony, while horizontal is the change from one time step to the next.</p>

<p>The idea of a coupled RNN-RBM is to cover both of these aspects:</p>
<ul>
  <li>The RNN generates content in the horizontal dimension</li>
  <li>The RBM models the probability distribution of melody/chord pairs in the vertical dimension.</li>
</ul>

<p>This strategy designs an archictecture based on an understanding of music theory (i.e. that melody and harmony support each other). In this way, it is unique to some of the other strategies that have been outlined, which begin from other perspectives.</p>

<p class="message">

Question for you: if music has two dimensions, that makes it just like an image right? Why can't we just apply existing image-based AI algorithms to a piano roll?

The surprisingly intuitive answer is that in music (and unlike in images), the vertical and horizontal dimensions are not equivalent. In a picture, both dimensions are length, but in a piano roll, one dimension is pitch and the other is time. 

That being said, Convolutional Neural Networks - which have been very effective for image processing - are not useless for music generation... we just have to find creative ways to apply it.

</p>

<h3 id="example-8---rnn-rnn-polyphany-generation">Example 8 - RNN-RNN Polyphany Generation</h3>

<p>This is nearly identical to the architecture in example 7, but the vertical dimension is modeled by another RNN, instead of an RBM.</p>

<blockquote>
  <p>For an example of this implementation, see the Bi-Axial LSTM archicture proposed by Daniel Johnson ‚ÄúGenerating Polyphonic music using tied parallel networks‚Äù (2017).</p>
</blockquote>

<h3 id="example-9---variational-recurrent-autoencoder">Example 9 - Variational Recurrent Autoencoder</h3>

<p>Besides the melody vs harmony distinction, another important challenge with music generation is that of musical <em>structure</em>. Songs are not one monolithic piece from start to finish, but rather are organized into sections, often with recurring motifs (e.g. AABA, Sonata, Etude, etc.).</p>

<p>Often, this type of capability is intentionally withheld from music writing AI‚Äôs because it‚Äôs very difficult to learn high level structures simultaneously with low-level harmony and melodic style.</p>

<p>MusicVAE is a variational recurrent autoencoder with a 2-level heirarchical RNN within the decoder. The introduction of an additional level improves the algorithm‚Äôs ability to learn structure. This allows it to generate longer sequences of music with better overall cohesion.</p>

<p>The first level within the hierarchy deals with low level concepts, like a musical phrase, while the second (higher) level deals with musical structure. (Technically, there is nothing that forces this association, but the hope is that by designing the archicture this way, the network will also learn this concept when it is trained).</p>

<h3 id="example-10---incorporation-of-dynamics">Example 10 - Incorporation of Dynamics</h3>

<p>Music played by a human rarely ends up sounding exactly like it is written on the page. Musicians will adjust the dynamics, and take liberties on tempo in an actual performance which are not captured by sheet music. These decisions are often made live, and are based on the musician‚Äôs understanding of the music and personal opinion.</p>

<p>One way to capture this information is to work directly with the audio waveforms rather than symbolic representation. Of course, the obvious downside to this strategy is that we lose the symbolic representation. A second alternative is to add this information to our training data as another aspect of the input and output.</p>

<blockquote>
  <p>In their program, called Performance RNN, Simon and Oore capture MIDI performance data and introduce an additional time shift and dynamic value to each time step. The time shift ranges from 10 ms to 1 sec, and the dynamics are captured by the 128 possible MIDI velocities quantized into 32 bins.</p>
</blockquote>

<h3 id="example-11---style-transfer">Example 11 - Style Transfer</h3>

<p>Style transfer is an active field of image processing, but hasn‚Äôt been explored very much in music. Musicians do this all the time in real life, (e.g. rearranging a pop song into a waltz, copying a jazz pianists style, or remixing a song into lo-fi). Conceptually, we can think about two types of style transfer:</p>

<ol>
  <li>Composition structure (e.g. time signature, song structure)</li>
  <li>Performance style (e.g. different style of harmony, embellishments. Think how different jazz pianists can approach the same standard in dramatically different ways)</li>
</ol>

<h3 id="example-12---control">Example 12 - Control</h3>

<p>Stepping back for a moment, let‚Äôs think about how we could apply generative AI. The first thing to realize is that most tools are designed to help a real person, and lack direction by themselves. Even the most advanced AI‚Äôs - like those that drive your car - still need a destination. So the primary question is: how can we exert control over the result?</p>

<p>One way we‚Äôve already discussed is through sampling. Like the predictive speech capability on your phone, non-deterministic algorithms are important because they provide <em>options</em>. But a big problem with sampling methods is that decoders only work in one direction. There‚Äôs no easy way to take the output and feed it back to learn a ‚Äúbetter‚Äù seed. Luckily, we are not without options.</p>

<p>For RNNs, one option we have is to threshold out low probability options (remember that notes are often predicted with a softmax function). This prevents these a bad note from propagating into futue time steps.</p>

<p>For single step feedforward networks, we can reduce the sampling scope. Instead of generating large chunks of music, we generate shorter phrases, or fewer voices within a larger and constrained piece.</p>

<h3 id="example-13---sampling-control-through-interpolation">Example 13 - Sampling Control through Interpolation</h3>

<p>So we mentioned earlier that decoders only allow information to pass through one way, but there‚Äôs a way around this: interpolation in the latent space.</p>

<blockquote>
  <p>The latent space is the space formed by the latent variables. (e.g. if our seed was 8452, there are 4 latent variables). We could perform clustering in the latent space to identify patterns to the seeds which produce our training data.</p>
</blockquote>

<p>In this sense, we aren‚Äôt really feeding information back through the decoder, but rather applying even more machine learning on the seeds to understand the significance of each digit. In other words, given a set of coordinate <em>numbers</em>, we‚Äôre learning if they are for a cartesian, polar, or spherical <em>system</em>.</p>

<blockquote>
  <p>Hadjeres and Nielsen discuss something called geodesic latent space regularization, which is supposed to be better than linear interpolation.</p>
</blockquote>

<h3 id="example-14---conditioning">Example 14 - Conditioning</h3>

<p>Conditioning an architecture means to bias it‚Äôs output, based on some extra information (typically applied as an additional input layer). For example, this could be a genre label, a chord progression, or maybe even the previously generated note.</p>

<h3 id="example-15---unit-selection">Example 15 - Unit Selection</h3>

<p>At its core, unit selection is an interative feedforward network, but instead of writing music note by note, it selects measures from a database, which when concatenated together, form a coherent musical idea. This approach needs metadata to represent the musical quality of each measure but unfortunately, music theory doesn‚Äôt really think of music in terms of measures. This means we as humans lack the language to think about what these pieces of metadata might be!</p>

<h3 id="example-16---originality">Example 16 - Originality</h3>

<p>For many skills, copying is easier than creating, and this is certainly true for generative AI as well. How can we bias our architecture to create original content, and not just poor copies? Importantly, we would like this to happen <em>a priori</em>, and not <em>a posteriori</em>.</p>

<p>The first approach involves a sort of <em>ad hoc</em> tuning of parameters within the archicture until we get results that mostly seem original. This can certainly be effective, but maybe lacks some of the intentionality we‚Äôre going for.</p>

<p>A second approach is a modification of the traditional generative adversarial network. In a traditional GAN, the classifier returns a signal of how easily/confidently it was able to classify the generated content as real or fake. The new approach, as outlined by Elgammal, called a <em>creative</em> adversarial network (CAN), uses these two contradictory forces:</p>

<ol>
  <li>The same signal in a traditional GAN about how easily it is able to distinguish between real and fake input.</li>
  <li>The ease with which it is able to classify the input into a learned style.</li>
</ol>

<p>Thus, the generative part of the network is incentived to create ouput that feels real, but also difficult to classify (hence, creative!). Elgammal‚Äôs work is primarily applicable to art, and has not yet been applied to music.</p>

<h3 id="example-17---refinement-over-time">Example 17 - Refinement over Time</h3>

<p>In the real world, your first draft is rarely your final draft. You‚Äôll go back through and make edits, but taking into account now your better understanding of where you‚Äôre coming from and where you‚Äôre going.</p>

<p>When applied to music generation and machine learning, this changes the input structure from maybe just a frame or structure, to something more constrained.</p>

<p>DeepBach is one such example, which uses two LSTM‚Äôs and two feedforward networks. The two LSTM‚Äôs approach from different sides of the music - one from the past and one from the future. A third network handles harmonic information, and the three together are passed into a final network for the output.</p>

<blockquote>
  <p>Incremental networks like the one just mentioned are great candidates for providing a user interface, and hence, control over the result!</p>
</blockquote>

<h3 id="example-18---adaptable-networks">Example 18 - Adaptable Networks</h3>

<p>Another common problem with networks is that they cannot learn after they‚Äôve been trained. How do you reinforce good patterns, and discourage bad ones? One option is to feed good outputs back into the training pool, but this risks over-fitting, and there is no practical way to punish bad behavior.</p>

<p>In practice, this kind of objective mostly has to be achieved by looking more closely at the generation step, and taking advantage of <em>control</em> strategies (like reinforcement, conditioning, constraints, etc.). Over-time, we can ‚Äúlearn‚Äù values for these parameters that improve the ouput.</p>

<h3 id="example-19---explainable-networks">Example 19 - Explainable Networks</h3>

<p>The challenge of explaining how AI ‚Äúthinks‚Äù is not unique to music. In general, deep learning architectures are a bit of a black box, but a few basic approaches have been tried to ‚Äúunderstand‚Äù what patterns each node, or collection of nodes is responding to.</p>

<blockquote>
  <p>This task gets a lot easier on simple inputs. For instance, in a low rank matrix, we can look at the vectors which span the matrix space, and understand that each vector captures one essential piece of information. But then again, if patterns were easy to spot, we wouldn‚Äôt need such complex deep learning archictures to begin with.</p>
</blockquote>

<ol>
  <li>Gradient sensitivity - seeing how changes to the inputs affect the result</li>
  <li>Signal isolation - attempting to isolate particular input patterns to trigger activation in high level neurons</li>
  <li>Attribution - looking at a specific location in the output, and seeing the contributions from each input dimension.</li>
</ol>
:ET