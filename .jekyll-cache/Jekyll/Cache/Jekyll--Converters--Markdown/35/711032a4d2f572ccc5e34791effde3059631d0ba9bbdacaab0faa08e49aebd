I"⁄<p>Machine vision refers broadly to any imaging analysis task performed by a computer. Meanwhile, simultaneous localization and mapping (SLAM) is a specific domain focused on obtaining an understanding of the environment and your position within it. These two often go hand in hand (but not always!).</p>

<!--more-->

<p>The topic of perception and machine vision can be broken down into a few separate problems.</p>

<ol>
  <li>Odometry ‚Äì where are you?*</li>
  <li>Reconstruction/Mapping ‚Äì where are things in the world?*</li>
  <li>Semantics ‚Äì what are things in the world?</li>
  <li>Prediction ‚Äì where will things in the world be?</li>
</ol>

<p><em>*The coordinate frame is very important! Positions are always defined with respect to something.</em></p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Implementation Stategy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Odometry</td>
      <td>Can be achieved with a number of things such as GPS, SLAM, internal IMU data, etc.</td>
    </tr>
    <tr>
      <td>Reconstruction</td>
      <td>Requires some sort of information about the environment. This can be achieved with cameras, LiDAR, radar, etc. There are also varying degrees to which we can process and store our internal model of the environment.</td>
    </tr>
    <tr>
      <td>Semantics</td>
      <td>Typically achieved through the use of machine learning, and some sort of image classification algorithm.</td>
    </tr>
    <tr>
      <td>Prediction</td>
      <td>Usually done with machine learning, or some kind of regression model.</td>
    </tr>
  </tbody>
</table>

<h3 id="image-projections">Image Projections</h3>

<p>Consider a set of points in 3D space, defined by the camera frame. The projection of this point into the image plane can be expressed as:</p>

\[u=f\frac{p_x^w}{p_z^w}\]

\[v=f\frac{p_y^w}{p_z^w}\]

<p>Where \(u\), \(v\) are the coordinates of point P expressed in the image plane, and f is the focal length of the camera. Note that \(u\), \(v\) are not pixel coordinates, but image plane coordinates defined from an origin.</p>

<p>An important characteristic of 2D images is the lack information regarding depth. A machine cannot tell if we are looking at a large object very far away or a close object nearby. There are multiple ways around this problem, including but not limited to LiDAR, Radar, or triangulation.</p>

<h3 id="feature-identification">Feature Identification</h3>

<p>Although modern vision systems may incorporate object identification and tracking, a more rudimentary system may begin by tracking local features. This begs the question, what defines a good feature? Long story short, corners are good features, while edges and surfaces are not.</p>

<p>Consider a rectangular window centered around some pixel \(\widetilde{x}\). This is a ‚Äúgood‚Äù feature if shifting the window in any direction produces a new window that is different from the original. Mathematically, let us define:</p>

\[G=\sum_{W\left(x\right)}^{\ }{\nabla I\left(\widetilde{x}\right)\nabla I\left(\widetilde{x}\right)^\top}\]

<p>Where \(\nabla I\left(\widetilde{x}\right)\) is the gradient of the image. Two common ways of scoring the corner is with either the Harris score, or the Shi-Tomasi score. These are, respectively:</p>

\[C\left(G\right)=det\left(G\right)-ktr\left(G\right)^2\]

\[S\left(G\right)=\lambda_{min}\left(G\right)\]

<p>Where \(\lambda_{min}\) is a function that returns the smallest eigenvalue of the matrix.</p>

<h3 id="outliers">Outliers</h3>

<p>When we use a quadratic loss function, the weights of outliers are exaggerated. Thus instead of fitting the correct data, we end up fitting to the incorrect ones. This is not an easy problem to overcome, because we do not know ahead of time which data points are the wrong ones, but a few outlier robust methods do exist.</p>

<h3 id="random-consensus-sampling-ransac">Random Consensus Sampling (RANSAC)</h3>

<p>From consecutive image frames, one can perform feature matching, and calculate the change in camera position between frames. However, given a set of matched features (typically called correspondences), there are bound to be a few mis-identified matches. RANSAC provides a method for dealing with these outliers.</p>

<p>The basic idea is pretty simple. From the entire complete set of ‚Äúmatches‚Äù, we take a random set, and calculate the resulting change in position. We then repeat with another random set, and another, and check to see if there is agreement.</p>

<h3 id="feature-tracking">Feature Tracking</h3>

<h3 id="image-segmentation">Image Segmentation</h3>

<p>Broadly, image segmentation is a form of pixel classification. The ultimate goal is to find meaningful groupings. Note that this can be done independently, in combination with, or as step in object classification, which involves finding labels. Traditional image segmentation methods are also not necessarily feature tracking methods or SLAM methods.</p>

<h3 id="simultaneous-localization-and-mapping-slam">Simultaneous Localization and Mapping (SLAM)</h3>

<p>SLAM is a process which covers the first two goals in computer vision - odometry and mapping. SLAM then can be further subdivided into two components, a <strong>front end</strong> which identifies features in the environment, and a <strong>back-end</strong> which builds an internal map of those features.</p>

<h3 id="misc-definitions">Misc Definitions</h3>

<p>Isomorphism ‚Äì a form of transformation that preserves structure.</p>

<p>Homography ‚Äì an isomorphism between projected images.</p>

:ET