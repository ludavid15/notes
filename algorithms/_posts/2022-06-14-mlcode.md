---
layout: post
title: Coding ML with Julia
permalink: /machineLearningCode/
mathjax: true
except_separator: <!--more-->
categories: algorithms
---

A short reference for using the Flux package in Julia to setup and train deep learning networks.

<!--more-->

### Basics

An efficient way to use one-hot encoding (better than regular arrays)

```
onehot(1, 1:5)
```

Softmax normalizes a onehot encoded vector such that the sum of all entries is exactly equal to 1 (as it should be for probability distributions)

```
softmax(model(xs[2]))
```

And now setting up a neural network:

```
model = Dense(2,3,sigma)
L(x,y) = Flux.mse(model(x),y)
opt = ADAM()
Flux.train!(L, zip(xs, ys), opt)
```

Where:
1. xs is the feature vector
2. ys is the label vector
3. L(x,y) is the loss function


### Deep Learning

We can build deep learning networks (i.e. networks with more than 1 layer) by using the Chain function.


```
layer1 = Dense(2, 3, sigma)
layer2 = Dense(3, 2, identity)
layer3 = softmax

model = Chain(layer1, layer2, layer3)  
L(x,y) = Flux.crossentropy(model(x), y)
opt = SGD(params(model))
```

### Batching

Batching is a function that merges many data points into a single matrix. This is more efficient to train because matrix-matrix multiplication has been heavily optimized.

```
databatch = (Flux.batch(xs), Flux.batch(ys))
Flux.train!(L, Iterators.repeated(databatch, 1000), opt)
```

Second line: Flux takes the model, and trains it on the dataset in databatch, and repeats 1000 times. 


### Callbacks

A function that we'd like Flux to call every so often. This is useful for capturing the loss or learning progress while we're training a model. We can wrap our callback function with the throttle function to set how often (in seconds).

```
Flux.throttle(callback, 1)
```