---
layout: post
title: Music and AI - Part 3
permalink: /musicAI3/
mathjax: true
except_separator: <!--more-->
categories: music
---

The thrilling conclusion to a three part series on music and artifical intelligence!

<!--more-->

### List of Architectures

1. Feedforward
2. Autoencoder
3. Variational Autoencoder
4. Restricted Boltzman Machine
5. Recurrent Networks
6. Convolutional Networks
7. Conditioning Convolutional Networks
8. Generative Adversarial Networks
9. Reinforcement Learning

### List of Strategies

1. Single step feedforward
2. Decoder feedforward
3. Sampling based methods
4. Iterative feedforward
5. Input manipulation
6. Reinforcement
7. Unit Selection


### Recurrent vs Convolutional Networks

Convolutional networks have not been explored thoroughly as a way to generate or recognize music. Maybe this is because music representation is so complex that it's difficult to visualize how a CNN would apply. 

That being said, convolutional networks do have two advantages:
1. Faster to train and easier to parallelize
2. By nature of their implementation, convolutional networks increase the volume of data.

### Transfer Learning

Transfer learning for music is one area that really hasn't been explored in depth, but which could be enormously useful, if we use the (more developed) subject of image generation as a sign of things to come. 


### Open Questions

1. Why do some training sets transpose all songs into a single key, while other training sets transpose all songs into every key? Is there a difference?

2. Can we use a lead sheet as the input, with a polyphonic melody/solo as the output? Maybe the pianist can be the conditional input (i.e. toggle style for different players)

3. Can we apply Elgammal's CAN, to create jazz solos that seem real, but which are not easily classified into an existing composer?